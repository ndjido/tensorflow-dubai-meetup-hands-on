{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Loading MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_dir = '/Users/ndjido/Documents/Davidson_Consulting/TF_DL_Meetup/Demo/ConvNet_MNIST/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev/' + name , stddev)\n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram/' + name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Network Building-block Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_conv_layer(inputs, inputs_channels_dim, patch_dim, output_channels_dim, stride_x=1, stride_y=1, name=\"ConvLayer\", activation=False):\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([patch_dim, patch_dim, inputs_channels_dim, output_channels_dim], stddev=0.1), name=\"W\")\n",
    "        variable_summaries(W, 'Weights')\n",
    "        b = tf.Variable(tf.truncated_normal([output_channels_dim]), name=\"b\")\n",
    "        variable_summaries(b, 'Biases')\n",
    "        \n",
    "        layer = tf.nn.conv2d(inputs, W, strides=[1, stride_x, stride_y, 1], padding=\"SAME\") + b\n",
    "        if activation == True:\n",
    "            return tf.nn.relu(layer)\n",
    "        else:\n",
    "            return layer\n",
    "        \n",
    "def create_polling_layer(conv_layer, _padding=\"SAME\", stride_x=2, stride_y=2, name=\"PollingLayer\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.relu(tf.nn.max_pool(conv_layer, ksize=[1, stride_x, stride_y, 1], strides=[1, stride_x, stride_y, 1], padding=_padding))\n",
    "\n",
    "def create_fully_connected_layer(inputs, inputs_dim, output_dim, name=\"fully_connected_layer\", p_dropout=0):\n",
    "     with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([inputs_dim, output_dim], stddev=0.1), name=\"W\")\n",
    "        variable_summaries(W, 'Weights')\n",
    "        b = tf.Variable(tf.truncated_normal([output_dim]), name=\"b\")\n",
    "        variable_summaries(b, 'Biases')\n",
    "        layer = tf.nn.softmax(tf.matmul(inputs, W) + b)\n",
    "        if p_dropout > 0.0 and p_dropout < 1.0:\n",
    "            return tf.nn.dropout(layer, p_dropout)\n",
    "        else:\n",
    "            return layer\n",
    "\n",
    "def create_softmax_layer(inputs, inputs_dim, output_dim, name=\"softmax_layer\"):\n",
    "     with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([inputs_dim, output_dim], stddev=0.1), name=\"W\")\n",
    "        variable_summaries(W, 'Weights')\n",
    "        b = tf.Variable(tf.truncated_normal([output_dim]), name=\"b\")\n",
    "        variable_summaries(b, 'Biases')\n",
    "        return tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input Dimensions\n",
    "A = 6\n",
    "B = 12\n",
    "C = 24\n",
    "D = 200\n",
    "\n",
    "nb_pixels_x = 28\n",
    "nb_pixels_y = nb_pixels_x\n",
    "_inputs_channels_dim = 1\n",
    "_output_dim = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, nb_pixels_x * nb_pixels_y])\n",
    "X_image = tf.reshape(X, [-1, nb_pixels_x, nb_pixels_y, _inputs_channels_dim])\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, _output_dim])   \n",
    "\n",
    "Y1 = create_conv_layer(X_image, _inputs_channels_dim, 6, A, 1, 1, name=\"ConvLayer_A\", activation=True)\n",
    "Y2 = create_conv_layer(Y1, A, 5, B, 2, 2, name=\"ConvLayer_B\", activation=True)\n",
    "Y3 = create_conv_layer(Y2, B, 4, C, 2, 2, name=\"ConvLayer_C\", activation=True)\n",
    "\n",
    "Y3_ = tf.reshape(Y3, shape=[-1, 7 * 7 * C])\n",
    "Y4 = create_fully_connected_layer(Y3_, 7 * 7 * C, D, name=\"fully_connected_layer\")\n",
    "\n",
    "Y = create_softmax_layer(Y4, D, _output_dim, name=\"softmax_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cross_entropy_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=Y_))\n",
    "\n",
    "tf.summary.scalar(\"cross_entropy_loss\", cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Trainer\"):\n",
    "    trainer = tf.train.AdamOptimizer(.03).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Summary writer for TensorBoad  \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(working_dir + 'train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(working_dir + 'test')\n",
    "    \n",
    "    for i in range(10000):\n",
    "        # Train\n",
    "        x_data, y_data = mnist.train.next_batch(100)\n",
    "        summary, _ = sess.run([merged_summary, trainer], feed_dict={X: x_data, Y_: y_data})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        # Test\n",
    "        summary, acc = sess.run([merged_summary, accuracy], feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('Step %s -> Accuracy %s' % (i, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy = 98.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
